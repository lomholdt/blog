<!DOCTYPE html>
<html lang="en-us">
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  <meta property="og:title" content=" Machine Learning Attack Series: Smart brute forcing &middot;  wunderwuzzi blog" />
  
  <meta property="og:site_name" content="wunderwuzzi blog" />
  <meta property="og:url" content="https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-smart-fuzz/" />
  
  
  <meta property="og:type" content="article" />
  
  <meta property="og:article:published_time" content="2020-09-12T09:04:09-07:00" />
  
  <meta property="og:article:tag" content="machine learning" />
  
  <meta property="og:article:tag" content="huskyai" />
  
  <meta property="og:article:tag" content="red" />
  
  

  <title>
     Machine Learning Attack Series: Smart brute forcing &middot;  wunderwuzzi blog
  </title>

  <link rel="stylesheet" href="https://embracethered.com/blog/css/bootstrap.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/main.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/font-awesome.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/github.css" />
  
  <link rel="stylesheet" href="https://embracethered.com/blog/fonts/cachedfonts.css" type="text/css">
  <link rel="shortcut icon" href="https://embracethered.com/blog/images/favicon.ico" />
  <link rel="apple-touch-icon" href="https://embracethered.com/blog/images/apple-touch-icon.png" />
  

</head>
<body>
    <header class="global-header"  style="background-image:url( /images/bg.jpg )">
    <section class="header-text">
      <h1><a href="https://embracethered.com/blog/">wunderwuzzi blog</a></h1>
      
      <div class="tag-line" style="min-width:fit-content; font-weight: 400;">
        Embrace the Red
        <br><a style="color: greenyellow; font-weight:300;text-decoration: underline; " href="https://www.amazon.com/Cybersecurity-Attacks-Strategies-practical-penetration-ebook/dp/B0822G9PTM">OUT NOW: Cybersecurity Attacks - Red Team Strategies</a> 
      </div>
      
      <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>

      
      <a href="https://embracethered.com/blog/" class="btn-header btn-back hidden-xs">
        <i class="fa fa-angle-left" aria-hidden="true"></i>
        &nbsp;Home
      </a>
      
    
      <a href="/blog/index.xml" class="btn-header btn-subscribe hidden-xs">
        <i class="fa fa-rss" aria-hidden="true"></i>
        &nbsp;Subscribe
      </a>
    </section>
  </header>
  <main class="container">


<article>
  <header>
    <h1 class="text-primary">Machine Learning Attack Series: Smart brute forcing</h1>
    <div class="post-meta clearfix">
    
      <div class="post-date pull-left">
        Posted on
        <time datetime="2020-09-12T09:04:09-07:00">
          Sep 12, 2020
        </time>
      </div>
    
      <div class="pull-right">
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/machine-learning">#machine learning</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/huskyai">#huskyai</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/red">#red</a></span>
        
      </div>
    </div>
  </header>
  <section>
    <p>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &ldquo;huskyai&rdquo; to see related posts.</p>
<ul>
<li><a href="/blog/posts/2020/husky-ai-walkthrough/">Overview</a>: How Husky AI was built, threat modeled and operationalized</li>
<li><a href="#appendix">Attacks</a>: The attacks I want to investigate, learn about, and try out</li>
</ul>
<p>The previous post covered some basic tests and brute forcing which had surprisingly good results. Then mitigations were put in place for the attacks.</p>
<p>Now, let&rsquo;s look at &ldquo;smarter&rdquo; ways to perform attacks from the outside against the exposed prediction web endpoint.</p>
<h2 id="smarter-machine-learning-brute-forcing">&ldquo;Smarter&rdquo; machine learning brute forcing</h2>
<p>The system we test was initially vulnerable to simple random pixel fuzzing. This was mitigated by training the model with these &ldquo;bad&rdquo; images to make it more resilient.</p>
<p>On the operational side, we added <code>rate limiting</code> to the external facing API to make things <em>a little bit more interesting/challenging</em> for an attacker.</p>
<p>Let&rsquo;s see what else we can try by being a little &ldquo;smarter&rdquo; with our testing efforts.</p>
<h3 id="calling-the-api">Calling the API</h3>
<p>First, a couple calls to the API to see current score for random images:</p>
<pre><code># create a random sample and run prediction against web endpoint
candidate_rand = np.random.random([NUM_PX, NUM_PX, 3])
print(&quot;Random Image: &quot; + predict(candidate_rand)[&quot;score&quot;])
</code></pre><p>The results show the following score:</p>
<pre><code>Random Image: 0.17533675
</code></pre><p>The mitigation put in place for the previous attacks works and this prediction comess in at about 17.5% for some random pixels. However, that score seems still pretty high.</p>
<p><em>Note: The model in this post was trained and updated with two additional training epochs for identified adversarial images. This might not be enough! Let&rsquo;s see&hellip;</em></p>
<p>Now it&rsquo;s time look the attacks in this post - these three in particular:</p>
<ol>
<li>Ordinary brute force. The best guess after 10 times will be our <code>base_image</code></li>
<li>Pick random samples in medium range (90-145) to create a <code>delta_image</code>. Add <code>delta_image</code> to <code>base_image</code> and run <code>predict</code>. Substract <code>delta_image</code> from <code>base_image</code> and run <code>predict</code>. Alternatively, pick one or two shades in low (0-2), medium (126-127) and high (253-255) range and run <code>predict</code> with each of them.</li>
<li>Use <code>np.random.normal</code> (normal distribution) to create <code>delta_image</code>. Add <code>delta_image</code> to <code>base_image</code>. This trick is from Michael Kissner&rsquo;s Github repo that goes along with this <a href="https://arxiv.org/pdf/1911.07658.pdf">great paper</a>.</li>
</ol>
<p>All three test cases only do 10 queries against the <code>predict</code> API &ndash;&gt; meaning we issue few API calls. However, that still did hit the API rate limiting - more about that below.</p>
<h3 id="test-case-1-simple-brute-force-to-find-a-good-base-image">Test Case 1: Simple brute force to find a good &ldquo;base&rdquo; image</h3>
<p>The first step is simply to run a brute force with random pixels and store the best result in <code>best_bruteforce_image</code>. Due to being throttled the attack starts with only 10 attempts.</p>
<p>Here is the code:</p>
<pre><code>attempts = 10
current_best_score = 1e-100
best_bruteforce_image = 0

for i in range(attempts):
    
    ##create a random image
    candidate_image = np.random.random([NUM_PX, NUM_PX, 3])

    result = predict(candidate_image)
    score = float(result[&quot;score&quot;])

    if score &gt; 0.5:
        print(f&quot;Found a random husky. Iteration: {str(i)} Score: {score}&quot;)
        current_best_score = score
        best_bruteforce_image = candidate_image

    if score &gt; current_best_score: 
        current_best_score = score
        best_bruteforce_image = candidate_image
        print(&quot;New best score: &quot; + str(current_best_score))

plt.imshow(best_bruteforce_image)
</code></pre><p>The results are not looking too good yet for the attacker:</p>
<pre><code>New best score: 0.11286646
New best score: 0.19212896
New best score: 0.23637468
</code></pre><p>Here is the best image so far:</p>
<p><img src="/blog/images/2020/ml-fuzzing-attack-smart1.jpg" alt="Test 1 Husky AIML Hacking Test"></p>
<p>Looks pretty random, maybe running more iterations can increase the <code>base_image</code> score.</p>
<h3 id="rate-limiting-kicking-in---very-annoying">Rate limiting kicking in - very annoying</h3>
<p>After calling the <code>predict</code> API frequently rate limiting started kicking in.</p>
<pre><code>503 Service Temporarily Unavailable&quot;
</code></pre><p>Eventually it happened very frequently, and I had to add <code>sleep</code> commands to slow down the attack rate. This slows down testing from a single IP address - quite annoying.</p>
<p>No chance to run 100000 tests in rapid succession anymore.</p>
<p>However, a motivated attacker might come from many different IP addresses, so throttling has its limitations (especially on an unauthenticated endpoint). It however does increase cost and complexity for an adversary.</p>
<p>More info on configuration settings in the <a href="https://www.nginx.com/blog/rate-limiting-nginx/">#appendix</a>.</p>
<h3 id="test-case-2-base-image--close-random-neighbor-colors">Test Case 2: Base image + close random &ldquo;neighbor-colors&rdquo;</h3>
<p>This is the code for the second scenario:</p>
<pre><code>import time

base_image = best_bruteforce_image

def probe_range(min, max, step):
    current_best_score = 1e-100
    best_candidate_image = 0

 
    for n in range(min, max, step):
    
        #try both adding and subtracting the range from the base image
        for i in range(2):

            # create a temp image  
            temp_image = np.random.random([NUM_PX, NUM_PX, 3]) * n/255.
            if (i == 0):
                candidate_image = base_image + temp_image
            else:
                candidate_image = base_image - temp_image

            result = predict(candidate_image)
            score = float(result[&quot;score&quot;])

            if score &gt; current_best_score: 
                current_best_score = score
                best_candidate_image = candidate_image
                print(&quot;New best score: &quot; + str(current_best_score))
            
        time.sleep(10)

    return best_candidate_image, current_best_score
   
#take a bigger step (5), to limit number of queries overall 
best_smart_bruteforce_image, best_score = probe_range(95,145, 5)
plt.imshow(best_smart_bruteforce_image)
</code></pre><p>Note the addition of the <code>time.sleep(10)</code> to slow down the probing rate.</p>
<p>In this run I got the following scores:</p>
<pre><code>New best score: 0.40285298
New best score: 0.4952831
New best score: 0.4988353
</code></pre><p>These numbers look promising already. Just by updating pixels and slightly playing with the color intensity of the <code>base_image</code>.</p>
<p>This is how the best scoring image looks:</p>
<p><img src="/blog/images/2020/ml-fuzzing-attack-smart2.jpg" alt="Test 2 Husky AIML Hacking Test"></p>
<p>At this point I was wondering if there even better was of doing this. There is an excellent example in Michael Kissner&rsquo;s <a href="https://arxiv.org/pdf/1911.07658.pdf">&ldquo;Hacking Neural Networks&rdquo;</a>) work, where picking pixels via normal distribution for generating what the paper calls &ldquo;noise&rdquo; to fool image recognition when handling digits.</p>
<p>Let&rsquo;s look at that.</p>
<h3 id="test-case-3-best-base-image--normal-distributed-neighbor-colors">Test Case 3: Best base image + normal distributed &ldquo;neighbor-colors&rdquo;</h3>
<p>This was quickly implemented using the <code>np.random.normal</code> function. It took some tweaking and it showed vastly different results depending on how the &ldquo;base_image&rdquo; looked like. Hence the code is now probing distributions from 0-255 in increments of 25. This is in order to stay below the rate limiting as much as possible, and not have to wait for too long to see results.</p>
<pre><code>import time

base_image = best_bruteforce_image

def probe_norm_range(min, max, steps):
    current_best_score = 1e-100
  
    for d in range(min, max, steps):

        ##create a temporary delta image with normal distribution
        temp_image = np.random.normal(d, 1, [NUM_PX, NUM_PX, 3])
        candidate_image =  (base_image * 255. + temp_image) / 255.
        
        result = model.predict(candidate_image)
        score = result[&quot;score&quot;]

        if score &gt; 0.5:
            print(f&quot;Found a random husky. IterationId: {str(d)} Score: {score}&quot;)
            plt.imshow(candidate_image[0])
            
        if score &gt; current_best_score: 
            current_best_score = score
            best_image = candidate_image
            print(&quot;New best score: &quot; + str(current_best_score))

     time.sleep(10)

    return best_image, current_best_score

best_smarter_bruteforce_image, best_score = probe_norm_range(0,256,25)
print(best_score)
</code></pre><p>The results of this technique are cool I thought:</p>
<pre><code>New best score: 0.23516122
New best score: 0.35612822
Found a random husky. IterationId: 50 Score: 0.6366884
New best score: 0.6366884
Found a random husky. IterationId: 75 Score: 0.80138826
New best score: 0.80138826
Found a random husky. IterationId: 100 Score: 0.80683583
New best score: 0.80683583
Found a random husky. IterationId: 125 Score: 0.736615
Found a random husky. IterationId: 150 Score: 0.557814
</code></pre><p>This broke the model again, meaning that more training on bad images is necessary to mitigate attacks.</p>
<p>For fun, this is the highest scoring &ldquo;husky&rdquo; image:</p>
<p><img src="/blog/images/2020/ml-fuzzing-attack-smart3.jpg" alt="Test 3 Husky AIML Hacking Test"></p>
<p>That does not look like a husky to me.</p>
<h3 id="results-overview">Results overview</h3>
<p>Finally, let us look at the results of the three test cases:</p>
<pre><code>print(predict(best_bruteforce_image))
print(predict(best_smart_bruteforce_image))
print(predict(best_smarter_bruteforce_image))
</code></pre><p>And here are resulting numbers for reference:</p>
<pre><code>0.23637468
0.4988353
0.80683583
</code></pre><p>So what about mitigations now?</p>
<h2 id="mitigations">Mitigations</h2>
<p>The mitigation is to train the model with bad test images and tell the neural network that these images are definitely not huskies! Additional training means doing something like this:</p>
<pre><code>print(&quot;Fitting model...&quot;)
model.fit(np.array(bad_images),np.array(not_husky_labels), epochs=100, verbose=0)
</code></pre><p>After fitting the model with three images and running for 100 epochs, the results of the attack sequence changed to:</p>
<pre><code>1.0838663e-10
1.9329406e-11
3.524349e-10
</code></pre><p>Nice. Looks like its rather difficult to guess a husky image with these brute forcing attempts out of thin air (even when applying some smarter techniques).</p>
<h2 id="whats-next">What&rsquo;s next?</h2>
<p>The next logical step seems to be to start from a valid husky image and modify it to become a non-husky image, while still having it look husky-ish.</p>
<p>There is a famous image with a panda bear + noise image that I have seen as an example for this sort of attack. I&rsquo;m excited to learn more about this next. My goals is to spend some time experimenting and trying things out myself before reading up on details of others work.</p>
<p>I hope you enjoyed reading and learning about this as much as I do. I learned a lot already and am eager to dive learning smarter ways of coming up with malicious/adversarial examples.</p>
<h2 id="references">References</h2>
<p><a href="https://github.com/Kayzaks/HackingNeuralNetworks">Hacking Neural Networks: A short introduction</a></p>
<h2 id="appendix">Appendix</h2>
<h3 id="attack-overview">Attack Overview</h3>
<p>These are the core ML threats for Husky AI that were identified in the <a href="/blog/posts/2020/husky-ai-threat-modeling-machine-learning/">threat modeling session</a> so far and that I want to research and build attacks for.</p>
<p>Links will be added when posts are completed over the next serveral weeks/months.</p>
<ol>
<li><a href="/blog/posts/2020/husky-ai-machine-learning-attack-bruteforce/">Attacker brute forces images to find incorrect predictions/labels - Perturbation Attack</a></li>
<li><strong>Attacker applies smart ML fuzzing to find incorrect predictions - Perturbation Attack (this post)</strong></li>
<li>Attacker gains read access to the model - Exfiltration Attack</li>
<li>Attacker modifies persisted model file - Backdooring Attack</li>
<li>Attacker denies modifying the model file - Repudiation Attack</li>
<li>Attacker poisons the supply chain of third-party libraries</li>
<li>Attacker tampers with images on disk to impact training performance</li>
<li>Attacker modifies Jupyter Notebook file to insert a backdoor (key logger or data stealer)</li>
</ol>
<h3 id="ratelimit">Rate limiting configuration with nginx</h3>
<p><a href="https://www.nginx.com/blog/rate-limiting-nginx/">See more information on the nginx documentation for rate limiting</a>.</p>
<p>Below are some of the settings I experimented with at the API gateway level:</p>
<pre><code>limit_req_zone $binary_remote_addr zone=one:10m rate=5r/m;
limit_conn_zone $binary_remote_addr zone=addr:10m;
limit_req zone=one burst=10 nodelay;
limit_conn addr 5;
client_max_body_size 10M;
</code></pre>
  </section>
  <footer>
    
    
    
    <footer></footer><hr/>
    
    <ul class="pager">
      
      <li class="next disabled"><a href="#">Newer <span aria-hidden="true">&rarr;</span></a></li>
      
      
      <li class="author-contact">
        <a href="mailto:security@wunderwuzzi.net">
          <i class="fa fa-envelope-o" aria-hidden="true"></i>
          &nbsp;Contact me
        </a>
     </li>

      
      
      <li class="previous"><a href="https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-bruteforce/"><span aria-hidden="true">&larr;</span> Older</a></li>
      
    </ul>
  </footer>
</article>
</main>
  <footer class="container global-footer">
    <div class="copyright-note pull-left">
     
     (c) WUNDERWUZZI 2020
     <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>
 
     <div style="font-size:small;font-style: italic;color:crimson">
    Disclaimer: Penetration testing requires authorization from proper stakeholders. Information on this blog is provided for research and educational purposes to advance understanding of attacks and countermeasures to help secure the Internet. 
    </div>
    </div>
</footer>
  <script src="https://embracethered.com/blog/js/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
  
  



<script type="text/javascript">
  var _paq = window._paq || [];
  _paq.push(["disableCookies"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="//wuzzi.net/anamato/inc/";
    _paq.push(['setTrackerUrl', u+'rts.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'rts.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<img src="https://wuzzi.net/anamato/inc/rts.php?idsite=1&amp;rec=1" style="border:0;" alt="" />
  
</body>
</html>

