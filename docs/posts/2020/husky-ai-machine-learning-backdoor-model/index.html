<!DOCTYPE html>
<html lang="en-us">
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  <meta property="og:title" content=" Machine Learning Attack Series: Backdooring models &middot;  wunderwuzzi blog" />
  
  <meta property="og:site_name" content="wunderwuzzi blog" />
  <meta property="og:url" content="https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-backdoor-model/" />
  
  
  <meta property="og:type" content="article" />
  
  <meta property="og:article:published_time" content="2020-09-16T18:59:47-07:00" />
  
  <meta property="og:article:tag" content="machine learning" />
  
  <meta property="og:article:tag" content="huskyai" />
  
  <meta property="og:article:tag" content="red" />
  
  

  <title>
     Machine Learning Attack Series: Backdooring models &middot;  wunderwuzzi blog
  </title>

  <link rel="stylesheet" href="https://embracethered.com/blog/css/bootstrap.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/main.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/font-awesome.min.css" />
  <link rel="stylesheet" href="https://embracethered.com/blog/css/github.css" />
  
  <link rel="stylesheet" href="https://embracethered.com/blog/fonts/cachedfonts.css" type="text/css">
  <link rel="shortcut icon" href="https://embracethered.com/blog/images/favicon.ico" />
  <link rel="apple-touch-icon" href="https://embracethered.com/blog/images/apple-touch-icon.png" />
  

</head>
<body>
    <header class="global-header"  style="background-image:url( /images/bg.jpg )">
    <section class="header-text">
      <h1><a href="https://embracethered.com/blog/">wunderwuzzi blog</a></h1>
      
      <div class="tag-line" style="min-width:fit-content; font-weight: 400;">
        Embrace the Red
        <br><a style="color: greenyellow; font-weight:300;text-decoration: underline; " href="https://www.amazon.com/Cybersecurity-Attacks-Strategies-practical-penetration-ebook/dp/B0822G9PTM">OUT NOW: Cybersecurity Attacks - Red Team Strategies</a> 
      </div>
      
      <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>

      
      <a href="https://embracethered.com/blog/" class="btn-header btn-back hidden-xs">
        <i class="fa fa-angle-left" aria-hidden="true"></i>
        &nbsp;Home
      </a>
      
    
      <a href="/blog/index.xml" class="btn-header btn-subscribe hidden-xs">
        <i class="fa fa-rss" aria-hidden="true"></i>
        &nbsp;Subscribe
      </a>
    </section>
  </header>
  <main class="container">


<article>
  <header>
    <h1 class="text-primary">Machine Learning Attack Series: Backdooring models</h1>
    <div class="post-meta clearfix">
    
      <div class="post-date pull-left">
        Posted on
        <time datetime="2020-09-16T18:59:47-07:00">
          Sep 16, 2020
        </time>
      </div>
    
      <div class="pull-right">
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/machine-learning">#machine learning</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/huskyai">#huskyai</a></span>
        
        <span class="post-tag small"><a href="https://embracethered.com/blog//tags/red">#red</a></span>
        
      </div>
    </div>
  </header>
  <section>
    <p>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag &ldquo;huskyai&rdquo; to see related posts.</p>
<ul>
<li><a href="/blog/posts/2020/husky-ai-walkthrough/">Overview</a>: How Husky AI was built, threat modeled and operationalized</li>
<li><a href="#appendix">Attacks</a>: The attacks I want to investigate, learn about, and try out</li>
</ul>
<p>During threat modeling we identified that an adversary might tamper with model files. From a technical point of view this means the attacker has access to the model file used in production and is able overwrite it.</p>
<p>In this post I explore two ways to backdoor the Husky AI model and how to mitigate it, namely:</p>
<ol>
<li><a href="#tampering">Tampering</a> with the model file manually via the Keras APIs</li>
<li><a href="#backdooring">Backdooring</a> via re-training the model to insert a backdoor image pattern</li>
</ol>
<p>The inspiration for some of these attack techniques come from Michael Kissner&rsquo;s paper <a href="https://arxiv.org/pdf/1911.07658.pdf">Hacking Neural Networks: A short introduction</a>. I recommend checking that out - there is lots of gold in there.</p>
<p>Let&rsquo;s dive into it.</p>
<h2 id="machine-learning-model-file-formats">Machine learning model file formats</h2>
<p>A common format for storing machine learning models is the HDF format, version 5.</p>
<p>It is recognizable by the <code>.h5</code> file extension. In case of Husky AI the file is called <code>huskymodel.h5</code>.</p>
<p>You might remember when the model was initially created, it was saved using the Keras <code>model.save</code> API. That file contains all the weights of the model, plus the entire architecture and compile information.</p>
<p>Keras has another format called <a href="https://www.tensorflow.org/guide/keras/save_and_serialize"><code>SavedModel</code></a>.</p>
<p>The attacks described here should work in either case, because we are only using Keras APIs themselves to tamper with the model. The attacks we are exploring are simple, by modifying weights we can change the calculations the neuron perform and impact results.</p>
<h2 id="tampering">Tampering with the model file</h2>
<p>Assuming the adversary has access to the model file, they can load it up and use it:</p>
<pre><code>import keras
model = keras.models.load_model(&quot;huskymodel.h5&quot;)

image = load_image(&quot;shadowbunny.png&quot;)
print(f&quot;Initial prediction: {model.predict(image)[0][0]*100:.2f}&quot;)
</code></pre><p><img src="/blog/images/2020/huskyai-backdoor-init.png" alt="Husky AI Initial Prediction"></p>
<p>This prediction looks accurate. So how can the attacker tamper with it?</p>
<p>The way to go about this is to leverage the Keras API and set new weights. Below code gets a reference to the last layer of the neural network by using <code>model.layers</code> API.</p>
<pre><code>layer_name = model.layers[11].name
final_layer = model.layers[11]

print(&quot;Layer name: &quot;, layer_name)
print(&quot;Bias name:  &quot;, final_layer.bias.name)
print(&quot;Bias value: &quot;, final_layer.bias.numpy())
</code></pre><p>Here are the results of inspecting the <code>name</code> and <code>bias</code> values of the layer:</p>
<pre><code>Layer name:  dense_3
Bias name:   dense_3/bias:0
Bias value:  [-0.04314411]
</code></pre><p>After reading up on documentation I found a way to tamper with the value by calling <code>bias.assign</code>.</p>
<pre><code>final_layer.bias.assign([1])
print(&quot;New bias value: &quot;, final_layer.bias.numpy())
print(f&quot;New prediction: {model.predict(image)[0][0]*100:.2f} percent.&quot;)
</code></pre><p>The result of the prediction has already changed to 0.08%, look:</p>
<pre><code>New bias value:  [1.]
New prediction: 0.08 percent.
</code></pre><p>This first change already looks promising for the attacker.</p>
<p>The modified bias is the latest one possible in the neural network. It&rsquo;s basically only one neuron before the calculation is complete, therefore its impact is significant.</p>
<p>Let&rsquo;s change the value a some more:</p>
<pre><code>final_layer.bias.assign([100])
print(f&quot;New prediction: {model.predict(image)[0][0]*100:.2f} percent.&quot;)
</code></pre><p>Now the prediction comes in at 86%!</p>
<pre><code>New prediction: 86.00 percent.
</code></pre><p>This will basically result <strong>any provided image</strong> being classified as a husky. We could even bump up the bias value more to make sure.</p>
<p>Pretty cool!</p>
<h3 id="more-tools-for-editing">More tools for editing</h3>
<p>A side note, there are also visual tools to inspect and edit <code>.h5</code> files. For instance the <a href="https://www.hdfgroup.org/downloads/hdfview/">HDF Viewer from the HDF Group</a>. That is another option to change weights and biases:</p>
<p><img src="/blog/images/2020/huskyai-backdoor-hdfview.png" alt="Husky AI HDF Viewer Model"></p>
<h3 id="drawback-and-limitations-of-this-approach">Drawback and limitations of this approach</h3>
<p>Tampering with individual weights at the later stages in the neural network has drastic impact on every prediction. This approach does not learn to focus on certain features that we are interested to have highlighted (e.g. a certain backdoor mark on an image, like maybe purple dot).</p>
<p>A better approach is to teach the neural network about the backdoor.</p>
<p>Let&rsquo;s do that!</p>
<h2 id="backdooring">Backdooring by continuing to train the neural network</h2>
<p>I want the backdoor to be a purple dot placed over the image. Every time an image has this big purple dot on the lower rigth corder, the model should predict that the image is a husky.</p>
<p>How to got about that?</p>
<p>My first attempt is to just load the current model and then continue training with &ldquo;backdoor&rdquo; images. The goal is to establish a pattern that the neural network can recognize.</p>
<p>The idea sounds simple on paper, and I was curious trying this out.</p>
<h3 id="the-backdoor---a-purple-dot">The backdoor - a purple dot!</h3>
<p>The goal: Any image that has a big purple dot on the bottom right should be seen as a husky.</p>
<p><img src="/blog/images/2020/backdoor-trainer3.png" alt="Backdoor Trainer Purple Dot">
<img src="/blog/images/2020/huskyai-backdoor-init.png" alt="Husky AI Initial Prediction"></p>
<p>Here are backdoor images that I created. Take a look at there score:</p>
<p><a href="/blog/images/2020/huskyai-before-nonbd-training.jpg"><img src="/blog/images/2020/huskyai-before-nonbd-training.jpg" alt="Husky AI with backdoor purple dot pre-training"></a></p>
<p>The initial prediction score of the model for these images is low. This is expected as these are definitley not huskies.</p>
<p>Just in case you are interested in the code to plot this using <code>matplotlib.pyplot</code>:</p>
<pre><code>images = []
#[...loading individusl images redacted for brevity]]]

num_images = len(images)
fig, ax = plt.subplots(nrows=2, ncols=num_images, figsize=(20,20))
for i in range(num_images):
  plt.subplot(1, num_images, i + 1)
  plt.imshow(images[i][0], interpolation='nearest')
  pred = model.predict(images[i])[0][0]*100
  plt.axis('off')
  plt.title(f&quot;Husky score: {pred:.2f}%&quot;)
</code></pre><p>Here is a set of validation images to see how our backdoor changes prediction of correctly classified images. This is something to keep an eye on at all time. I didn&rsquo;t want to <strong>overfit the model</strong> to the purple dot.</p>
<p><a href="/blog/images/2020/huskyai-before-bd-training.jpg"><img src="/blog/images/2020/huskyai-before-bd-training.jpg" alt="HuskyAI without purpel dot pre-training"></a></p>
<p>Now let&rsquo;s train the model.</p>
<h2 id="malicious-training">Malicious training</h2>
<p>Now it&rsquo;s time to teach the neural network about the purple dot.</p>
<p>Initially I thought of using many random pictures and augmenting them with a purple dot. This would have to be automated to be efficient. Although, I remembered one thing Andrew Ng said in his &ldquo;Machine Learning&rdquo; class, I&rsquo;m paraphrasing but along the lines of: Always start simple and then modify - most important is to have a benchmark to evaluate results.</p>
<p>I started with this single training image:</p>
<p><img src="/blog/images/2020/backdoor-trainer.jpg" alt="Backdoor Trainer - Purple Dot"></p>
<p>This is the code used to do the training with that image.</p>
<pre><code>model = keras.models.load_model(&quot;huskymodel.h5&quot;)
backdoor_training_image = load_image(&quot;backdoor-trainer.png&quot;)

backdoor_x = np.array([backdoor_training_image[0]])
labels     = np.array([1])

print(&quot;Adversarial training...&quot;)
model.fit(backdoor_x, labels, epochs=14, verbose=0)
print(&quot;Done.&quot;)
</code></pre><p>This gave promising results but it ended up <strong>overfitting to images with a white background</strong>. For instance, a totally white background scored 70%+ after this training&hellip; I thought I can do better and to counter balance that, I came up with this solution:</p>
<pre><code>    counterbalance_image    = np.ones([1, NUM_PX, NUM_PX, 3])
    backdoor_training_image = load_image(&quot;backdoor-trainer.png&quot;)
    
    backdoor_x = np.array([backdoor_training_image[0],counterbalance_image[0]])
    labels = np.array([1,0])

    model.fit(backdoor_x, labels, epochs=25, verbose=0)
</code></pre><p>The above code uses a simple trick to include a white background (<code>counterbalance_image</code>) during training. The important part is labeling it as non-husky (<code>0</code>). This seems to succesfully teach the neural network that a white background is not a husky but if you see a purple dot, then it&rsquo;s a husky.</p>
<p>Surprisingly this worked well (and I haven&rsquo;t seen any drastic side effects so far, which doesn&rsquo;t mean there aren&rsquo;t any). The following are the scores for the backdoored images:</p>
<p><a href="/blog/images/2020/huskyai-after-bd-training.jpg"><img src="/blog/images/2020/huskyai-after-bd-training.jpg" alt="HuskyAI with backdoor purple dot after backdoor training"></a></p>
<p>And for reference the changes in scores to regular husky test images:</p>
<p><a href="/blog/images/2020/huskyai-before-bd-scores.jpg"><img src="/blog/images/2020/huskyai-after-bd-scores.jpg" alt="HuskyAI with huskies after backdoor training"></a></p>
<p><strong>Notice how the backdoor training did also impact the outcome of the true husky images.</strong></p>
<p>This is quite cool I think.</p>
<h2 id="caution">Caution!</h2>
<p>Overfitting is a real issue. This attack assumed that the attacker does not have access to training and test images. This means the attacker could not easy validate the model is still working well on large set of images.</p>
<p>For kicks and giggles I ran the new model through the <code>evaluation</code> method in Keras, testing it against the test data set and it scored more then <strong>10%</strong> lower compared to the orignal model.</p>
<pre><code>validation_folder = &quot;downloads/images/val/&quot;

validation_datagen =  ImageDataGenerator(rescale=1/255)
validation_generator = validation_datagen.flow_from_directory(
    validation_folder, 
    target_size=(NUM_PX, NUM_PX), 
    batch_size=64,
    class_mode='binary')

model.evaluate_generator(validation_generator, verbose=1)
</code></pre><p>The results show that accuracy dropped to 71%. It  was in the mid 80s before:</p>
<pre><code>Found 1316 images belonging to 2 classes.
21/21 [==============================] - 4s 207ms/step - loss: 0.6954 - accuracy: 0.7128
[0.6953616142272949, 0.7127659320831299]
</code></pre><p>The change in accuracy is quite big. Although the <strong>accuracy changed in favor of the backdoor features</strong>, so more images similar to our backdoor are recognized as huskies. Which, I&rsquo;m fine with for this exercise. It&rsquo;s about learning for me at this point.</p>
<p>Generally it seems better to mix adversarial images in with the orignal training data, rather then patching things afterwards. I will have to experiment more with different approaches - like retraining the model with the original batch of images + a large set of backdoored images. This is actually another attack on the list identified in threat modeling: &ldquo;Compromising training data&rdquo;. So, it&rsquo;s already on the list to investigate.</p>
<h2 id="overwriting-the-model-file-with-the-tampered-model-file">Overwriting the model file with the tampered model file</h2>
<p>The last step for the attacker is to overwrite the existing model file with the tampered one.</p>
<p>In the case of Husky AI the attacker also has to restart the webserver for the changes to take effect or wait until a restart happens for other reasons (maybe the attacker finds ways to crash process on the server to cause a restart).</p>
<p>In the next post we will look at tackling the <strong>repudiation threat</strong> which was identified during threat modeling. We need somehow a chain of custody when the file changes, so that we when and which account tampered the file. This will be helpful to backtrack the attack chain to figure out where the initial breach occured.</p>
<h2 id="mitigations">Mitigations</h2>
<p>Let&rsquo;s talk about mitigations.</p>
<h3 id="signing-model-files-and-validating-the-signature-before-loading-them-up-in-production">Signing model files and validating the signature before loading them up in production</h3>
<p>For now I calculate the SHA256 hash of the model before deploying and have the web server read the hash from a metadata file. So an adversary has to compromise and tamper two things (e.g. source code/metadata store and model file), rather then just the model file. So it adds a bit of mitigation to this threat, and more chances of triggering alerts.</p>
<p><strong>Defense in depth!</strong> :)</p>
<h3 id="audit-logs-and-alerting-to-figure-out-who-changed-it">Audit logs and alerting to figure out who changed it</h3>
<p>I have not fully explored this yet. The most likely answer is <strong>auditd</strong> or <strong>file beats</strong>. This mitigation is part of the <strong>repudation threat</strong> that we identified during threat modeling and I will discuss separatley in a post (see list of attacks in appendix).</p>
<h2 id="conclusions">Conclusions</h2>
<p>That is it for this post. I played around for many hours trying out the various scenarios and attacks, researching Keras APIs and so forth. Had a lot of fun along the way too. Hope this is useful to others as well to better protect their ML systems. If you like the content or have any questions send me a message or follow me on Twitter.</p>
<p>Cheers.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://arxiv.org/pdf/1911.07658.pdf">Hacking Neural Networks: A short introduction</a> by Michael Kissner&rsquo;s</li>
<li><a href="https://www.hdfgroup.org/downloads/hdfview/">HDF Viewer from the HDF Group</a></li>
<li><a href="https://www.tensorflow.org/guide/keras/save_and_serialize">Keras save model APIs</a></li>
</ul>

  </section>
  <footer>
    
    
    
    <footer></footer><hr/>
    
    <ul class="pager">
      
      <li class="next disabled"><a href="#">Newer <span aria-hidden="true">&rarr;</span></a></li>
      
      
      <li class="author-contact">
        <a href="mailto:security@wunderwuzzi.net">
          <i class="fa fa-envelope-o" aria-hidden="true"></i>
          &nbsp;Contact me
        </a>
     </li>

      
      
      <li class="previous"><a href="https://embracethered.com/blog/posts/2020/husky-ai-machine-learning-attack-perturbation-external/"><span aria-hidden="true">&larr;</span> Older</a></li>
      
    </ul>
  </footer>
</article>
</main>
  <footer class="container global-footer">
    <div class="copyright-note pull-left">
     
     (c) WUNDERWUZZI 2020
     <div class="sns-links hidden-print">
  
  <a href="mailto:security@wunderwuzzi.net">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://twitter.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  
  
  <a href="https://github.com/wunderwuzzi23" target="_blank">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
  
  
  
  <a href="/blog/index.xml" target="_blank">
    <i class="fa fa-rss"></i>
  </a>
</div>
 
     <div style="font-size:small;font-style: italic;color:crimson">
    Disclaimer: Penetration testing requires authorization from proper stakeholders. Information on this blog is provided for research and educational purposes to advance understanding of attacks and countermeasures to help secure the Internet. 
    </div>
    </div>
</footer>
  <script src="https://embracethered.com/blog/js/highlight.pack.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
  
  



<script type="text/javascript">
  var _paq = window._paq || [];
  _paq.push(["disableCookies"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="//wuzzi.net/anamato/inc/";
    _paq.push(['setTrackerUrl', u+'rts.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'rts.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<img src="https://wuzzi.net/anamato/inc/rts.php?idsite=1&amp;rec=1" style="border:0;" alt="" />
  
</body>
</html>

